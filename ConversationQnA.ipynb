{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996ad9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "\n",
    "########################################## model (use groq) #####################################################\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model = ChatGroq(model_name=\"Llama3-8b-8192\",groq_api_key = groq_api_key)\n",
    "print(\"MODEL ---->\",model)\n",
    "\n",
    "\n",
    "######################################### embedding model ######################################################\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv(\"huggin_face_token\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"embeddings ---->\",embeddings)\n",
    "\n",
    "####################################### load documents from the web ###############################################3\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import bs4 \n",
    "loader = WebBaseLoader(\n",
    "    web_paths= (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\",\"post-title\",\"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(\"docs --->\",docs)\n",
    "\n",
    "######################################### chunking ###############################################################\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "########################################## convert docs into embeddings and store into vectorstore #################\n",
    "vectorstore = Chroma.from_documents(documents = splits,embedding=embeddings)\n",
    "print(\"vectorstore ----->\",vectorstore)\n",
    "\n",
    "######################################### convert chroma into langchain runnable retriever #######################\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"retriever ------>\",retriever)\n",
    "\n",
    "######################################### prompt template ##############################################\n",
    "system_prompt = (\"\"\" You are an assistant for question-answering tasks. \n",
    "                 Use the following pieces of retrieved context to answer the question. \n",
    "                 If you don't know the answer, say that you don't know. \n",
    "                 Use three sentences maximum and keep the answer concise.\\n\\n\n",
    "                 {context}\n",
    "                 \"\"\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "######################################## create retrieval chain (text summarization - stuff type) ###############################################\n",
    "question_answer_chain = create_stuff_documents_chain(model,prompt)\n",
    "reg_chain = create_retrieval_chain(retriever,question_answer_chain)\n",
    "response = reg_chain.invoke({\"input\":\"what is self-reflection?\"})\n",
    "print(\"response ---->\",response)\n",
    "\n",
    "\n",
    "\n",
    "################################### Adding chat History along with prompt template ####################################################################\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder \n",
    "\n",
    "## Prompt 1 - Reformulate user query with chat history, input is chat_history + input, will be used by create_history_aware_retriever()\n",
    "contextualize_q_system_prompt = (\"\"\" Given a chat history and the latest user question which might reference context in the history, formulate a standalone question which can be understood without the chat history. \n",
    "                                 DO NOT answer the question, just reformulate it if needed and otherwise return as is.\n",
    "                                 \"\"\")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),       \n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "## Prompt 2 - Answer question based on retrieved chunks, input is context + chat_history + input, will be used by create_stuff_documents_chain()\n",
    "qa_system_prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "System_Prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",qa_system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(model,retriever,contextualize_q_prompt)\n",
    "print(\"history_aware_retriever ------->\",history_aware_retriever)\n",
    "\n",
    "question_answer_chain_with_history = create_stuff_documents_chain(model,System_Prompt)\n",
    "rag_chain_with_history = create_retrieval_chain(history_aware_retriever,question_answer_chain_with_history)\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "chat_history = []\n",
    "question = \"what is task decomposition?\"\n",
    "response1 = rag_chain_with_history.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "print(\"type of response1\",type(response1))\n",
    "print(\"response1 --->\",response1[\"answer\"])\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content = question),\n",
    "    AIMessage(content = response1[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "question2 = \"provide detail explaination on the subject?\"\n",
    "response2 =  rag_chain_with_history.invoke({\"input\":question2,\"chat_history\":chat_history})\n",
    "print(\"response2 ---->\",response2[\"answer\"])\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content = question),\n",
    "    AIMessage(content = response2[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "###################################################### hitory with session ids ######################################\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "# To separate the sessions\n",
    "def get_session_history(session_id)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()  # creating an object of ChatMessageHistory for each session\n",
    "    return store[session_id]         # returning ChatMessageHistory object of that particular session\n",
    "\n",
    "# ------- creating 2 sessions -------\n",
    "config1={\"configurable\":{\"session_id\":\"Chat_1\"}}   # configurable must be smallcase here\n",
    "config2={\"configurable\":{\"session_id\":\"Chat_2\"}}\n",
    "\n",
    "conversation_rag_chain = RunnableWithMessageHistory(rag_chain_with_history,get_session_history,input_messages_key=\"input\",history_messages_key=\"chat_history\",output_messages_key=\"answer\")\n",
    "\n",
    "response1_session1 = conversation_rag_chain.invoke({\"input\":\"what is chain of Thought?\"},config=config1)[\"answer\"]\n",
    "print(\"response1_session1  ->\", response1_session1)\n",
    "\n",
    "response1_session2 = conversation_rag_chain.invoke({\"input\":\"what is Tree of Thoughts?\"},config=config2)[\"answer\"]\n",
    "print(\"response1_session2 -> \",response1_session2)\n",
    "\n",
    "response2_session1 = conversation_rag_chain.invoke({\"input\":\"Please elaborate on the topic?\"},config=config1)[\"answer\"]\n",
    "print(\"response2_session1 ->\",response2_session1)\n",
    "\n",
    "response2_session2 = conversation_rag_chain.invoke({\"input\":\"Please elaborate on the topic?\"},config=config2)[\"answer\"]\n",
    "print(\"response2_session2 ->\",response2_session2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844f9f1",
   "metadata": {},
   "source": [
    "# Text Summarization \n",
    "there are 3 types of text summarization or 3 common ways to combine retrieved documents before passing them to the LLM:\n",
    "    1. Stuff -> All documents are concatenated and sent in a single prompt.\n",
    "    2. Map-Reduce -> Each document is processed individually and then results are combined.\n",
    "    3. Refine -> An answer is built iteratively, updating with each new doc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9d8aa",
   "metadata": {},
   "source": [
    "## Explaination of above code\n",
    "we are setting up a sentence transformer model (all-MiniLM-L6-v2) for converting text into vector embeddings. Used WebBaseLoader to load a blog post on autonomous agents from Lilian Weng’s site. Documents are too big to embed directly or pass to an LLM, so we split them into smaller chunks. Chroma is a fast and simple in-memory vector database which is then converted into a retriever. \n",
    "In Prompt it defines \"system\" message giving the assistant instructions (what to do with the context), and \"human\" message that provides the actual user query ({input}). {context} will be populated with the retrieved document chunks.\n",
    "\n",
    "\"create_stuff_documents_chain\" builds a simple RAG chain using the “stuff” method:\n",
    "    All retrieved documents are stuffed into one input along with the user’s question.\n",
    "    Best for shorter documents or small k (e.g., 2–5 retrieved chunks)\n",
    "\n",
    "create_retrieval_chain connects:\n",
    "    Your retriever (searches Chroma)\n",
    "    Your LLM + prompt chain\n",
    "\n",
    "invoke step triggers the full process:\n",
    "    Query → Retriever → Stuffed prompt → LLM → Answer\n",
    "\n",
    "#### About {Context}\n",
    "The {context} placeholder in your prompt template is populated automatically by LangChain, based on the document chunks retrieved by your retriever.\n",
    "Prompt Template -> \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),  # system_prompt includes {context}\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "{context} is expected in the system message\n",
    "{input} is expected in the human message (i.e., the question)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "reg_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "create_retrieval_chain() wires together:\n",
    "    A retriever (searches for relevant docs)\n",
    "    A document-processing chain (i.e., question_answer_chain)\n",
    "\n",
    "when invoked, response = reg_chain.invoke({\"input\": \"what is self-reflection?\"}) \n",
    "Use \"input\" (\"what is self-reflection?\") as the question, Passes that to the retriever, The retriever returns a list of relevant Document objects.\n",
    "LangChain automatically:\n",
    "    Extracts their page_content, Concatenates them into one string (e.g., separated by newlines), Substitutes that string into the {context} variable in your prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228bf42",
   "metadata": {},
   "source": [
    "# Explaination of adding history code\n",
    "We need to provide 2 prompts when history is involved, one for adding chat history to current query and other for question answering.\n",
    "\n",
    "Prompt 1 — Query Reformulation (require -> chat_history + input)\n",
    "Reformulates a follow-up question using previous chat history.\n",
    "Used in create_history_aware_retriever().\n",
    "\n",
    "Prompt 2 — Final Answer Prompt (require -> chat_history + input + context)\n",
    "Uses retrieved context to generate the final response.\n",
    "Used in create_stuff_documents_chain().\n",
    "\n",
    "History-Aware Retriever\n",
    "    history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
    "QA Chain with Retrieved Chunks\n",
    "    question_answer_chain_with_history = create_stuff_documents_chain(model, System_Prompt)\n",
    "Final RAG Chain\n",
    "    rag_chain_with_history = create_retrieval_chain(history_aware_retriever, question_answer_chain_with_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd1bb7",
   "metadata": {},
   "source": [
    "# Explaination of history with session code\n",
    "\n",
    "# BaseMessageHistory \n",
    "An abstract base class that defines the interface for chat history storage.\n",
    "\n",
    "## Common implementations\n",
    "1-> ChatMessageHistory (in-memory)\n",
    "2-> FileMessageHistory (local storage)\n",
    "3-> RedisChatMessageHistory (Redis-backed) etc etc \n",
    "\n",
    "# ChatMessageHistory\n",
    "A concrete in-memory implementation of BaseMessageHistory. It stores messages as a list of LangChain BaseMessage objects.\n",
    "\n",
    "# RunnableWithMessageHistory\n",
    "A wrapper that lets you add memory (chat history) to any LangChain Runnable — e.g., a prompt → LLM chain.\n",
    "It's typically used to make a stateless LLM chain behave like a stateful chatbot.\n",
    "\n",
    "get_session_history creates a session manager for chat history, Each session (Chat_1, Chat_2, etc.) gets its own independent message history stored in store.\n",
    "\n",
    "conversation_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain_with_history,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "This wraps your rag_chain_with_history so that:\n",
    "    It automatically manages message history per session.\n",
    "        Adds: \"input\" from user, \"chat_history\" for context, \"answer\" from model, All wrapped neatly into one Runnable.\n",
    "\n",
    "esponse1_session1 = conversation_rag_chain.invoke(\n",
    "    {\"input\": \"what is chain of Thought?\"}, config=config1\n",
    ")[\"answer\"]\n",
    "First call uses session \"Chat_1\" → stores that exchange in store[\"Chat_1\"]\n",
    "\n",
    "response1_session2 = conversation_rag_chain.invoke(\n",
    "    {\"input\": \"what is Tree of Thoughts?\"}, config=config2\n",
    ")[\"answer\"]\n",
    "Second call uses \"Chat_2\" → separate memory\n",
    "\n",
    "\n",
    "## History is automatically retrieved and updated by LangChain behind the scenes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
